{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1324e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow and NumPy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "# Define model architecture\n",
    "class DCGMNet(tf.keras.Model):\n",
    "    \"\"\" Set basic architecture of the model.\"\"\"\n",
    "\n",
    "    def __init__(self, X_low, X_high,\n",
    "                 input_dim, output_dim,\n",
    "                 n_layers_FFNN, layer_width,\n",
    "                 activation_FFNN,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.X_low = X_low\n",
    "        self.X_high = X_high\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.n_layers_FFNN = n_layers_FFNN\n",
    "        self.layer_width = layer_width\n",
    "        \n",
    "        self.activation_FFNN = activation_FFNN\n",
    "        # print(activation_FFNN)\n",
    "        \n",
    "        # Define NN architecture\n",
    "        # self.initial_scale = tf.keras.layers.Lambda(\n",
    "        #     lambda x: 2.0*(x - X_low)/(X_high - X_low) - 1.0)\n",
    "        self.initial_scale = tf.keras.layers.Dense(layer_width)\n",
    "        \n",
    "        self.hidden = [tf.keras.layers.Dense(layer_width,\n",
    "                                             activation=tf.keras.activations.get(\n",
    "                                                 activation_FFNN),\n",
    "                                             kernel_initializer=kernel_initializer)\n",
    "                       for _ in range(self.n_layers_FFNN)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"Forward-pass through neural network.\"\"\"\n",
    "        Z = self.initial_scale(X)\n",
    "        for i in range(self.n_layers_FFNN):\n",
    "            Z = self.hidden[i](Z) + Z\n",
    "        return self.out(Z)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0581e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aiyagari problem parameters (TensorFlow constants)\n",
    "kappa = tf.constant(0.5, dtype=tf.float32)  # mean reversion rate\n",
    "theta = tf.constant(0.0, dtype=tf.float32)  # mean reversion level\n",
    "sigma = tf.constant(2.0, dtype=tf.float32)  # volatility\n",
    "\n",
    "# Mean and standard deviation for (normally distributed) process starting value\n",
    "alpha = tf.constant(0.0, dtype=tf.float32)\n",
    "beta = tf.constant(1.0, dtype=tf.float32)\n",
    "\n",
    "nSim_x_interior = tf.constant(1000, dtype=tf.int32)  # Number of interior samples\n",
    "\n",
    "# Replace NumPy arrays with TensorFlow tensors\n",
    "X_low = tf.constant([-8.0], dtype=tf.float32)  # wealth lower bound\n",
    "X_high = tf.constant([8.0], dtype=tf.float32)  # wealth upper bound\n",
    "\n",
    "# Neural network parameters\n",
    "num_layers_FFNN = 4\n",
    "num_layers_RNN = 0\n",
    "nodes_per_layer =50\n",
    "starting_learning_rate = 0.001\n",
    "shrinkstep = 20000\n",
    "shrinkcoef = 0.95\n",
    "activation_FFNN = 'tanh'  # Activation function remains a string\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages =2000  # Number of resampling stages\n",
    "steps_per_sample = 10  # SGD steps per resampling\n",
    "\n",
    "dim_input = 1\n",
    "dim_output = 1\n",
    "\n",
    "# Define the model\n",
    "model = DCGMNet(X_low, X_high,\n",
    "                dim_input, dim_output,  # Convert TensorFlow constants to Python integers\n",
    "                num_layers_FFNN, nodes_per_layer,\n",
    "                activation_FFNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39244460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateOU_GaussianStart(theta, kappa, sigma, nSim):\n",
    "    ''' Simulate end point of Ornstein-Uhlenbeck process with normally \n",
    "        distributed random starting value.\n",
    "    \n",
    "    Args:\n",
    "        alpha: mean of random starting value\n",
    "        beta:  standard deviation of random starting value\n",
    "        theta: mean reversion level\n",
    "        kappa: mean reversion rate\n",
    "        sigma: volatility \n",
    "        nSim:  number of simulations\n",
    "        T:     terminal time        \n",
    "    '''  \n",
    "        \n",
    "    # simulate initial point based on normal distribution\n",
    "    \n",
    "    # mean and variance of OU endpoint\n",
    "    m = theta \n",
    "    v = np.sqrt(sigma**2 / ( 2 * kappa) )\n",
    "    \n",
    "    # simulate endpoint\n",
    "    X = np.random.normal(m,v,size=(nSim,1))    \n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "# def sampler(nSim_x_interior):\n",
    "#     ''' Sample time-space points from the function's domain; points are sampled\n",
    "#         uniformly on the interior of the domain, at the initial/terminal time points\n",
    "#         and along the spatial boundary at different time points. \n",
    "    \n",
    "#     Args:\n",
    "#         nSim_x_interior: number of space points in the interior of the function's domain to sample \n",
    "#     ''' \n",
    "    \n",
    "#     # Sampler #1: domain interior\n",
    "#     x_interior = np.random.uniform(low=X_low[0], high=X_high[0], size=[nSim_x_interior, 1])\n",
    "    \n",
    "#     return x_interior\n",
    "\n",
    "def sampler(nSim_x_interior):\n",
    "    ''' Sample time-space points from the function's domain; points are sampled\n",
    "        uniformly on the interior of the domain, at the initial/terminal time points\n",
    "        and along the spatial boundary at different time points. \n",
    "    \n",
    "    Args:\n",
    "        nSim_x_interior: number of space points in the interior of the function's domain to sample \n",
    "    ''' \n",
    "    \n",
    "    # Use TensorFlow's random uniform sampling\n",
    "    x_interior = tf.random.uniform(\n",
    "        shape=[nSim_x_interior, 1],\n",
    "        minval=X_low[0],\n",
    "        maxval=X_high[0],\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    \n",
    "    return x_interior\n",
    "\n",
    "\n",
    "def compute_loss(model, x_interior):\n",
    "    ''' Compute total loss for training.\n",
    "        NOTE: the loss is based on the PDE satisfied by the negative-exponential\n",
    "              of the density and NOT the density itself, i.e. the u(t,x) in \n",
    "              p(t,x) = exp(-u(t,x)) / c(t)\n",
    "              where p is the density and c is the normalization constant\n",
    "    \n",
    "    Args:\n",
    "        model:      DGM model object\n",
    "        t:          sampled (interior) time points\n",
    "        x_interior: sampled space points in the interior of the function's domain\n",
    "        x_initial:  sampled space points at initial time\n",
    "        nSim_t:     number of (interior) time points sampled (size of t)\n",
    "        alpha:      mean of normal distribution for process starting value\n",
    "        beta:       standard deviation of normal distribution for process starting value\n",
    "    ''' \n",
    "    \n",
    "    # Loss term #1: PDE\n",
    "    \n",
    "\n",
    "    # for each simulated interior time point\n",
    "        \n",
    "    # make vector of current time point to align with simulated interior space points   \n",
    "    x_interior = tf.cast(tf.reshape(x_interior, shape=[\n",
    "                            nSim_x_interior, 1]), tf.float32)\n",
    "    # print(t_vector)\n",
    "    # compute function value and derivatives at current sampled points\n",
    "    u    = model(tf.stack([x_interior[:,0]], axis=1))\n",
    "    u_x = tf.gradients(u, x_interior)[0]\n",
    "    u_xx = tf.gradients(u_x, x_interior)[0]\n",
    "\n",
    "    # psi function: normalized and exponentiated neural network\n",
    "    # note: sums are used to approximate integrals (importance sampling)\n",
    "    # psi_denominator = tf.reduce_sum(tf.exp(-u))\n",
    "    # print(u_t)\n",
    "\n",
    "    # PDE differential operator\n",
    "    # NOTE: EQUATION IN DOCUMENT IS INCORRECT - EQUATION HERE IS CORRECT\n",
    "    diff_f = kappa - kappa*(x_interior - theta)*u_x + 0.5*sigma**2* ( -u_xx + u_x**2)\n",
    "    \n",
    "    # compute L2-norm of differential operator and attach to vector of losses\n",
    "    currLoss = tf.reduce_mean(tf.square(diff_f)) \n",
    "\n",
    "    # average losses across sample time points \n",
    "    L1 = currLoss\n",
    "    \n",
    "    # Loss term #2: boundary condition\n",
    "        # no boundary condition for this problem\n",
    "    \n",
    "    # Loss term #3: initial condition\n",
    "    \n",
    "    # compute negative-exponential of neural network-implied pdf at t = 0\n",
    "    # i.e. the u in p = e^[-u(t,x)] / c(t)\n",
    "    \n",
    "    \n",
    "    # init_t = tf.cast(0, tf.float32)\n",
    "    # x_interior = tf.cast(tf.reshape(x_interior, shape=[\n",
    "    #     nSim_x_interior, 1]), tf.float32)\n",
    "    # t_vector = init_t * tf.ones_like(x_interior)\n",
    "    \n",
    "    # fitted_pdf = model(\n",
    "    #     tf.stack([0*tf.ones_like(x_initial)[:,0], x_initial[:,0]], axis=1))\n",
    "    # # target pdf - normally distributed starting value\n",
    "    # # NOTE: we are only comparing the exponential terms \n",
    "    # target_pdf = tf.cast(0.5*(x_initial - alpha)**2 / (beta**2),tf.float32)\n",
    "    \n",
    "    # # average L2 error for initial distribution\n",
    "    # L3 = tf.reduce_mean(tf.square(fitted_pdf - target_pdf))\n",
    "\n",
    "\n",
    "    return L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5f0b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def get_grad(model, x_interior):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        tape.watch(model.trainable_variables)\n",
    "        loss1 = compute_loss(model, x_interior)\n",
    "        loss = loss1\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    del tape\n",
    "    \n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b305470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\super\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:735: UserWarning: Gradients do not exist for variables ['dcgm_net_4/dense_29/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_74172\\1425131784.py\", line 7, in train_step  *\n        loss, grad_theta = get_grad(model, x_interior)\n    File \"C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_74172\\1029912891.py\", line 5, in get_grad  *\n        tape.watch(model.trainable_variables)\n    File \"c:\\Users\\super\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py\", line 356, in __repr__\n        value = backend.core.convert_to_numpy(self._value)\n    File \"c:\\Users\\super\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py\", line 161, in convert_to_numpy\n        return np.array(x)\n\n    NotImplementedError: numpy() is only available when eager execution is enabled.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m x_interior \u001b[38;5;241m=\u001b[39m sampler(nSim_x_interior)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps_per_sample):\n\u001b[1;32m---> 22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_step(model, x_interior)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Append the loss tensor directly\u001b[39;00m\n\u001b[0;32m     25\u001b[0m hist\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[1;32mc:\\Users\\super\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileizbiv8sg.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(model, x_interior)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m loss, grad_theta \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(get_grad), (ag__\u001b[38;5;241m.\u001b[39mld(model), ag__\u001b[38;5;241m.\u001b[39mld(x_interior)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(grad_theta), ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filezk7agj0k.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__get_grad\u001b[1;34m(model, x_interior)\u001b[0m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 11\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mwatch, (ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtrainable_variables,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m     loss1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(compute_loss), (ag__\u001b[38;5;241m.\u001b[39mld(model), ag__\u001b[38;5;241m.\u001b[39mld(x_interior)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(loss1)\n",
      "File \u001b[1;32mc:\\Users\\super\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:356\u001b[0m, in \u001b[0;36mVariable.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_value\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 356\u001b[0m     value \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mconvert_to_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value)\n\u001b[0;32m    357\u001b[0m value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, value=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<Variable path=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\super\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:161\u001b[0m, in \u001b[0;36mconvert_to_numpy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mRaggedTensor):\n\u001b[0;32m    160\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto_tensor()\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(x)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_74172\\1425131784.py\", line 7, in train_step  *\n        loss, grad_theta = get_grad(model, x_interior)\n    File \"C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_74172\\1029912891.py\", line 5, in get_grad  *\n        tape.watch(model.trainable_variables)\n    File \"c:\\Users\\super\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py\", line 356, in __repr__\n        value = backend.core.convert_to_numpy(self._value)\n    File \"c:\\Users\\super\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py\", line 161, in convert_to_numpy\n        return np.array(x)\n\n    NotImplementedError: numpy() is only available when eager execution is enabled.\n"
     ]
    }
   ],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=starting_learning_rate)\n",
    "# optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=starting_learning_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=starting_learning_rate)\n",
    "@tf.function\n",
    "def train_step(model, x_interior):\n",
    "    # Compute current loss and gradient w.r.t. parameters\n",
    "    loss, grad_theta = get_grad(model, x_interior)\n",
    "\n",
    "    # Perform gradient descent step\n",
    "    optimizer.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "hist = []\n",
    "\n",
    "for i in range(sampling_stages):\n",
    "    # Sample uniformly from the required regions\n",
    "    x_interior = sampler(nSim_x_interior)\n",
    "\n",
    "    for _ in range(steps_per_sample):\n",
    "        loss = train_step(model, x_interior)\n",
    "\n",
    "    # Append the loss tensor directly\n",
    "    hist.append(loss)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        tf.print(i, loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b07a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# vector of x values for plotting\n",
    "x_plot = np.linspace(X_low, X_high, 1000)\n",
    "\n",
    "\n",
    "\n",
    "# simulate process at current t\n",
    "sim_x = simulateOU_GaussianStart(\n",
    "    theta, kappa, sigma,nSim_x_interior*100)\n",
    "\n",
    "\n",
    "x_plot = tf.cast(tf.reshape(x_plot, shape=[\n",
    "    1000, 1]), tf.float32)\n",
    "\n",
    "u = model(tf.stack([x_plot[:, 0]], axis=1))\n",
    "u = u.numpy().reshape(-1,1)\n",
    "\n",
    "p = np.exp(-u)\n",
    "# p = u\n",
    "x_plot_orig = np.linspace(X_low[0], X_high[0], 1000)\n",
    "# print()\n",
    "\n",
    "density = p/np.trapz(p.reshape(x_plot_orig.shape), x_plot_orig)\n",
    "# density = p\n",
    "\n",
    "\n",
    "# plot histogram of simulated process values and overlay estimated density\n",
    "plt.hist(sim_x, bins=40, density=True, color='b')\n",
    "plt.plot(x_plot, density, 'r', linewidth=2.5)\n",
    "\n",
    "# subplot options\n",
    "plt.ylim(ymin=0.0, ymax=0.45)\n",
    "plt.xlabel(r\"$x$\", fontsize=15, labelpad=10)\n",
    "plt.ylabel(r\"$p(t,x)$\", fontsize=15, labelpad=20)\n",
    "\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
